# Extractive Question-Answering by BERT

Question Answering using BERT (Bidirectional Encoder Representations from Transformers) has significantly advanced natural language understanding and information retrieval. As a cutting-edge transformer model, BERT effectively captures contextual relationships in text through its bidirectional attention mechanism. In QA tasks, BERT analyzes both a question and the associated context to create embeddings for each word, considering the complete surrounding context. The model then identifies the relevant answer span within that context, allowing it to comprehend and extract pertinent information. When fine-tuned for question answering, BERT has shown outstanding performance across various datasets and is widely utilized in applications such as search engines, virtual assistants, and information retrieval systems, improving the accuracy and efficiency of extracting precise answers from text. In this notebook, the BERT model is fine-tuned using the SQUAD 2.0 question and answering dataset obtained from Kaggle.
